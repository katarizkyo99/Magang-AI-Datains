# -*- coding: utf-8 -*-
"""Chatbot Gradio _Rizky Octa Vianto.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IpQP_VS6hoDfqEhGybZs98ClvSqxCaSA
"""

import gradio as gr
import torch

from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer
from threading import Thread

# Load model dan tokenizer
tokenizer = AutoTokenizer.from_pretrained("togethercomputer/RedPajama-INCITE-Chat-3B-v1")
model = AutoModelForCausalLM.from_pretrained("togethercomputer/RedPajama-INCITE-Chat-3B-v1", torch_dtype=torch.float16)
model = model.to('cuda:0')

# Stopping criteria class
class StopOnTokens(StoppingCriteria):
    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        stop_ids = [29, 0]
        return any(input_ids[0][-1] == stop_id for stop_id in stop_ids)

# System prompt
SYSTEM_PROMPT = """
Anda adalah chatbot AI yang responsif dan membantu.
Jawablah pertanyaan dengan jelas dan akurat.
Jika informasi tidak tersedia, beri tahu pengguna bahwa Anda tidak yakin.
Jaga agar respons tetap ringkas dan informatif.
"""

def predict(message, history):
    history_transformer_format = history + [[message, ""]]
    stop = StopOnTokens()

    # Menyusun sistem prompt + percakapan sebelumnya
    messages = SYSTEM_PROMPT + "".join(["".join(["\n<human>:" + item[0], "\n<bot>:" + item[1]])
                for item in history_transformer_format])

    model_inputs = tokenizer([messages], return_tensors="pt").to("cuda")
    streamer = TextIteratorStreamer(tokenizer, timeout=10., skip_prompt=True, skip_special_tokens=True)
    generate_kwargs = dict(
        model_inputs,
        streamer=streamer,
        max_new_tokens=512,
        do_sample=True,
        top_p=0.95,
        top_k=1000,
        temperature=0.8,
        num_beams=1,
        stopping_criteria=StoppingCriteriaList([stop])
    )

    t = Thread(target=model.generate, kwargs=generate_kwargs)
    t.start()

    partial_message = ""
    for new_token in streamer:
        if new_token != '<':
            partial_message += new_token
            yield partial_message

# Membuat UI Gradio yang lebih jelas
demo = gr.ChatInterface(
    predict,
    title="AI Chatbot 1",
    description="Chatbot AI",
    theme="compact"
)

demo.launch()